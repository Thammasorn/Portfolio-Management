{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import random\n",
    "from keras.models import load_model\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "code_folding": [
     1,
     22,
     32,
     79,
     82,
     100
    ]
   },
   "outputs": [],
   "source": [
    "class environment:\n",
    "    def __init__(self, stock_1 ,stock_2 ,capital ,window_size, moving_average_size):\n",
    "        self.action_space = [0.25,0.1,0.05,0,0.05,0.1,0.25]\n",
    "        self.stocks = [stock_1,stock_2]\n",
    "        self.capital = capital\n",
    "        # Create feature of both stock\n",
    "        self.feature, self.close_price, self.open_price = self.create_feature(window_size,moving_average_size,stock_1,stock_2)\n",
    "        # Get the id of last day in data\n",
    "        self.last_day_number = self.close_price.shape[0]-1\n",
    "        ### Build the variable\n",
    "        ### *** Note that the value will be assigned in reset function\n",
    "        ###    - day id \n",
    "        self.day = 0\n",
    "        ###    - Random initial portfolio\n",
    "        self.initial_portfolio = 0\n",
    "        ###    - Create the number of share of each stock\n",
    "        self.amount_share = 0\n",
    "        ###    - Current portfolio value\n",
    "        self.current_portfolio_value = 0\n",
    "        ###    - done\n",
    "        self.done = False\n",
    "        \n",
    "    def reset(self):\n",
    "        self.day = 0#random.randint(0,self.last_day_number)\n",
    "        self.done = False\n",
    "        self.initial_portfolio = np.array([0.5,0.5])#np.random.dirichlet(np.ones(2))\n",
    "        self.amount_share = self.capital*self.initial_portfolio/self.open_price[self.day]\n",
    "        self.current_portfolio_value = self.cal_portfolio_value_at_the_end_of_day()\n",
    "        #### Create state \n",
    "        state = np.hstack((self.feature[self.day],self.current_portfolio_value/np.sum(self.current_portfolio_value)))\n",
    "        return state\n",
    "        \n",
    "    def step(self,action_id):\n",
    "        action = self.action_space[action_id]\n",
    "        ### Calculate the portfolio value according to the action\n",
    "        if action_id<3:\n",
    "            ### If action_id <3, then sell x% of values of stock 1 and buy the corresponding amount of stock2 \n",
    "            adjust_amount = self.current_portfolio_value[0]*action\n",
    "            new_portfolio_fraction = [ self.current_portfolio_value[0] - adjust_amount\\\n",
    "                                      ,self.current_portfolio_value[1] + adjust_amount]\n",
    "        elif action_id>3:\n",
    "            ### If action_id >3, then sell x% of values of stock 2 and buy the corresponding amount of stock1 \n",
    "            adjust_amount = self.current_portfolio_value[1]*action\n",
    "            new_portfolio_fraction = [ self.current_portfolio_value[0] + adjust_amount\\\n",
    "                                      ,self.current_portfolio_value[1] - adjust_amount]\n",
    "        else:\n",
    "            ### If action_id = 3, then do nothing\n",
    "            new_portfolio_fraction = self.current_portfolio_value\n",
    "            \n",
    "        ### Shift to next day\n",
    "        self.day += 1\n",
    "        ###    - if the final day in data have been reached, then done = true\n",
    "        if self.day == self.last_day_number: self.done = True\n",
    "            \n",
    "        ### Update portfolio (buy at open price of the day)\n",
    "        ###      **** Note that the open price is the close price of the previous day\n",
    "        self.amount_share = new_portfolio_fraction/self.open_price[self.day]\n",
    "        \n",
    "        ### Calculate reward\n",
    "        ###    - Store old portfolio value\n",
    "        total_old_portfolio_value = np.sum(self.current_portfolio_value)\n",
    "        ###    - Calculate new portfolio value\n",
    "        self.current_portfolio_value = self.cal_portfolio_value_at_the_end_of_day()\n",
    "        ###    - Calculate reward\n",
    "        total_current_portfolio_value = np.sum(self.current_portfolio_value)\n",
    "        reward =  100.0*(total_current_portfolio_value-total_old_portfolio_value)/total_old_portfolio_value\n",
    "        \n",
    "        ### Create state\n",
    "        ###    - stock feature\n",
    "        stock_feature = self.feature[self.day]\n",
    "        ###    - proportion feature\n",
    "        proportion_feature = self.current_portfolio_value/np.sum(self.current_portfolio_value)\n",
    "        ###    - merge features\n",
    "        next_state = np.hstack((self.feature[self.day],proportion_feature))\n",
    "        \n",
    "        return next_state,reward,self.done\n",
    "    \n",
    "    def cal_portfolio_value_at_the_end_of_day(self):\n",
    "        return self.amount_share*self.close_price[self.day]\n",
    "\n",
    "    def feature_of_stock(self, stock, window_size, moving_average_size):\n",
    "        ### Read data and select only close price\n",
    "        price = pd.read_csv(stock)[['Close']][:3200]\n",
    "        ### Assume that open price is the close price of previous day\n",
    "        price['Open'] = price.shift(1)\n",
    "        ### Calculate the EMA of price for reducing the noise in series\n",
    "        price['EMA'] = price['Open'].ewm(span=moving_average_size, adjust=False).mean()\n",
    "        ### Lag the ema price for (window_size+1) time\n",
    "        for i in range(window_size+1):\n",
    "            price['EMA_lag{}'.format(i)] = price['EMA'].shift(i)\n",
    "        ### Create percent change between each consecutive day (0-100%)\n",
    "        for i in range(window_size):\n",
    "            price['percent_change_EMA{}'.format(i+1)] = (price['EMA_lag{}'.format(i)]-price['EMA_lag{}'.format(i+1)])*100\\\n",
    "                                                        /price['EMA_lag{}'.format(i+1)]\n",
    "        price = price.dropna()\n",
    "        ### Select only the percent change as the feature\n",
    "        feature = np.array(price[['percent_change_EMA{}'.format(i+1) for i in range(window_size)]].values.tolist())\n",
    "        ### Return feature as feature, \n",
    "        ###        price['close'] as the sell price at that day,\n",
    "        ###        price[lag] as a buy price at that day\n",
    "        return feature,price['Close'],price['Open']\n",
    "    \n",
    "    def create_feature(self,\\\n",
    "                       window_size = 7,\\\n",
    "                       moving_average_size = 3,\\\n",
    "                       stock1 = '../../Thesis/Data/high_volatile/APA.csv',\\\n",
    "                       stock2 = '../../Thesis/Data/low_volatile/JNJ.csv'):\n",
    "        ### Generate feature of each stock\n",
    "        feature_1,close_price_1,open_price_1 = self.feature_of_stock(stock1,window_size,moving_average_size)\n",
    "        feature_2,close_price_2,open_price_2 = self.feature_of_stock(stock2,window_size,moving_average_size)\n",
    "        ### Concat feature from 2 stocks to be one features\n",
    "        feature = np.concatenate([feature_1,feature_2],axis=1)\n",
    "        ### Concat buy price and sell price to format [[<buyorsell>ofstock1,<buyorsell>ofstock2], [],[]]\n",
    "        ### example of buy price\n",
    "        ###                 [[buy price of stock 1 in day 0, buy price of stock 2 in day 0]\n",
    "        ###                  [buy price of stock 1 in day 1, buy price of stock 2 in day 2],....]\n",
    "        close_price = np.stack((close_price_1,close_price_2),axis=-1)\n",
    "        open_price = np.stack((open_price_1,open_price_2),axis=-1)\n",
    "        return feature,close_price,open_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            try:\n",
    "                target_f[0][action] = target\n",
    "            except:\n",
    "                print \"Target {}\".format(target)\n",
    "                raise ValueError('A very specific bad thing happened.')\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    window_size = 7\n",
    "    env = environment('../../Thesis/Data/high_volatile/APA.csv','../../Thesis/Data/low_volatile/JNJ.csv',100000000.0,7,3)\n",
    "    state_size = window_size*2 + 2\n",
    "    action_size = 7\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "    for e in range(EPISODES):\n",
    "        print e\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        while True:\n",
    "            # env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "        agent.model.save('model/{}.h5'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
